"""
Prometheus metrics for the RAG pipeline.
"""
from prometheus_client import Counter, Histogram, Gauge, Info
from typing import Dict, Any

REQUEST_COUNT = Counter(
    'rag_requests_total',
    'Total number of RAG requests',
    ['method', 'endpoint', 'status']
)

REQUEST_DURATION = Histogram(
    'rag_request_duration_seconds',
    'Request duration in seconds',
    ['method', 'endpoint']
)

ERROR_COUNT = Counter(
    'rag_errors_total',
    'Total number of errors',
    ['error_type', 'endpoint']
)

LLM_REQUESTS = Counter(
    'rag_llm_requests_total',
    'Total number of LLM requests',
    ['model', 'provider']
)

LLM_TOKENS_GENERATED = Counter(
    'rag_llm_tokens_generated_total',
    'Total tokens generated by LLM',
    ['model', 'provider']
)

LLM_TOKENS_CONSUMED = Counter(
    'rag_llm_tokens_consumed_total',
    'Total tokens consumed by LLM',
    ['model', 'provider']
)

LLM_RESPONSE_TIME = Histogram(
    'rag_llm_response_time_seconds',
    'LLM response time in seconds',
    ['model', 'provider']
)

VECTOR_STORE_QUERIES = Counter(
    'rag_vector_store_queries_total',
    'Total number of vector store queries',
    ['store_type', 'operation']
)

VECTOR_STORE_RESPONSE_TIME = Histogram(
    'rag_vector_store_response_time_seconds',
    'Vector store response time in seconds',
    ['store_type', 'operation']
)

DOCUMENTS_INDEXED = Counter(
    'rag_documents_indexed_total',
    'Total number of documents indexed',
    ['store_type']
)

DOCUMENTS_RETRIEVED = Counter(
    'rag_documents_retrieved_total',
    'Total number of documents retrieved',
    ['store_type']
)

DOCUMENT_CHUNKS = Gauge(
    'rag_document_chunks_total',
    'Current number of document chunks',
    ['store_type']
)

CACHE_HITS = Counter(
    'rag_cache_hits_total',
    'Total number of cache hits',
    ['cache_type']
)

CACHE_MISSES = Counter(
    'rag_cache_misses_total',
    'Total number of cache misses',
    ['cache_type']
)

CACHE_OPERATIONS = Counter(
    'rag_cache_operations_total',
    'Total number of cache operations',
    ['cache_type', 'operation']
)

SAFETY_CHECKS = Counter(
    'rag_safety_checks_total',
    'Total number of safety checks',
    ['scanner_type', 'result']
)

SAFETY_VIOLATIONS = Counter(
    'rag_safety_violations_total',
    'Total number of safety violations',
    ['violation_type', 'severity']
)

DOCUMENTS_PROCESSED = Counter(
    'rag_documents_processed_total',
    'Total number of documents processed',
    ['document_type', 'status']
)

PROCESSING_TIME = Histogram(
    'rag_processing_time_seconds',
    'Document processing time in seconds',
    ['document_type', 'operation']
)

CHUNK_PROCESSING_TIME = Histogram(
    'rag_chunk_processing_time_seconds',
    'Chunk processing time in seconds',
    ['operation']
)

ACTIVE_CONNECTIONS = Gauge(
    'rag_active_connections',
    'Number of active connections'
)

MEMORY_USAGE = Gauge(
    'rag_memory_usage_bytes',
    'Memory usage in bytes',
    ['component']
)

DISK_USAGE = Gauge(
    'rag_disk_usage_bytes',
    'Disk usage in bytes',
    ['component']
)

APP_INFO = Info(
    'rag_app_info',
    'Application information'
)

APP_INFO.info({
    'version': '1.0.0',
    'environment': 'development',
    'component': 'full-rag-pipeline'
})


class MetricsCollector:
    """Metrics collector for the RAG pipeline."""

    def __init__(self):
        self.active_requests = 0
        self.total_requests = 0
        self.total_errors = 0

    def record_request(self, method: str, endpoint: str, status: int, duration: float):
        """Record a request."""
        REQUEST_COUNT.labels(method=method, endpoint=endpoint, status=status).inc()
        REQUEST_DURATION.labels(method=method, endpoint=endpoint).observe(duration)

        self.total_requests += 1
        if status >= 400:
            self.total_errors += 1

    def record_llm_request(self, model: str, provider: str, tokens_in: int, tokens_out: int, duration: float):
        """Record an LLM request."""
        LLM_REQUESTS.labels(model=model, provider=provider).inc()
        LLM_TOKENS_CONSUMED.labels(model=model, provider=provider).inc(tokens_in)
        LLM_TOKENS_GENERATED.labels(model=model, provider=provider).inc(tokens_out)
        LLM_RESPONSE_TIME.labels(model=model, provider=provider).observe(duration)

    def record_vector_operation(self, store_type: str, operation: str, duration: float):
        """Record a vector store operation."""
        VECTOR_STORE_QUERIES.labels(store_type=store_type, operation=operation).inc()
        VECTOR_STORE_RESPONSE_TIME.labels(store_type=store_type, operation=operation).observe(duration)

    def record_document_indexed(self, store_type: str, chunk_count: int):
        """Record document indexing."""
        DOCUMENTS_INDEXED.labels(store_type=store_type).inc()
        DOCUMENT_CHUNKS.labels(store_type=store_type).inc(chunk_count)

    def record_documents_retrieved(self, store_type: str, count: int):
        """Record document retrieval."""
        DOCUMENTS_RETRIEVED.labels(store_type=store_type).inc(count)

    def record_cache_operation(self, cache_type: str, operation: str, hit: bool = None):
        """Record cache operation."""
        CACHE_OPERATIONS.labels(cache_type=cache_type, operation=operation).inc()

        if hit is not None:
            if hit:
                CACHE_HITS.labels(cache_type=cache_type).inc()
            else:
                CACHE_MISSES.labels(cache_type=cache_type).inc()

    def record_safety_check(self, scanner_type: str, result: str):
        """Record safety check."""
        SAFETY_CHECKS.labels(scanner_type=scanner_type, result=result).inc()

    def record_safety_violation(self, violation_type: str, severity: str):
        """Record safety violation."""
        SAFETY_VIOLATIONS.labels(violation_type=violation_type, severity=severity).inc()

    def record_document_processing(self, document_type: str, status: str, duration: float):
        """Record document processing."""
        DOCUMENTS_PROCESSED.labels(document_type=document_type, status=status).inc()
        PROCESSING_TIME.labels(document_type=document_type, operation="process").observe(duration)

    def update_system_metrics(self, active_connections: int, memory_usage: Dict[str, int], disk_usage: Dict[str, int]):
        """Update system metrics."""
        ACTIVE_CONNECTIONS.set(active_connections)

        for component, usage in memory_usage.items():
            MEMORY_USAGE.labels(component=component).set(usage)

        for component, usage in disk_usage.items():
            DISK_USAGE.labels(component=component).set(usage)

    def get_summary(self) -> Dict[str, Any]:
        """Get metrics summary."""
        return {
            'total_requests': self.total_requests,
            'total_errors': self.total_errors,
            'error_rate': self.total_errors / max(self.total_requests, 1) * 100,
            'active_requests': self.active_requests
        }


metrics_collector = MetricsCollector()